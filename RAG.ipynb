{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work/anaconda3/envs/lc/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "from langchain.embeddings import CacheBackedEmbeddings,HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.retrievers import BM25Retriever,EnsembleRetriever\n",
    "from langchain.document_loaders import PyPDFLoader,DirectoryLoader\n",
    "from langchain.cache import InMemoryCache\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.callbacks import StdOutCallbackHandler\n",
    "from langchain import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用LangChain进行数据解析和加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of documents in :117\n"
     ]
    }
   ],
   "source": [
    "dir_loader = DirectoryLoader(\"/home/work/project/MockingBird-main/pdf_data/\",\n",
    "                             glob=\"*.pdf\",\n",
    "                             loader_cls=PyPDFLoader)\n",
    "docs = dir_loader.load()\n",
    "print(f\"len of documents in :{len(docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用 RecursiveCharacterTextSplitter 创建文本片段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of chunks in barbie documents : 330\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=200)\n",
    "\n",
    "esops_documents = text_splitter.transform_documents(docs)\n",
    "print(f\"number of chunks in barbie documents : {len(esops_documents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 向量化存储\n",
    "\n",
    "- 在这里，将利用CacheBackedEmbeddings来防止我们一遍又一遍地重新嵌入类似的查询。\n",
    "- 结构化文档将被转换为有用的格式，以便在大模型应用中查询、检索和使用\n",
    "- 这里将使用 FAISS（Facebook AI 相似性搜索）作为向量检索引擎。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name /home/work/var/data/ssr-share-data/m3e-base/. Creating a new one with MEAN pooling.\n"
     ]
    }
   ],
   "source": [
    "store = LocalFileStore(\"./cache/\")\n",
    "# 提前下载到本地的m3e向量化模型\n",
    "embed_model_id = '/home/work/var/data/ssr-share-data/m3e-base/'\n",
    "core_embeddings_model = HuggingFaceEmbeddings(model_name=embed_model_id)\n",
    "embedder = CacheBackedEmbeddings.from_bytes_store(core_embeddings_model,\n",
    "                                                  store,\n",
    "                                                  namespace=embed_model_id)\n",
    "# Create VectorStore\n",
    "vectorstore = FAISS.from_documents(esops_documents,embedder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 创建检索器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_retriever = BM25Retriever.from_documents(esops_documents)\n",
    "bm25_retriever.k=5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 从向量库中检索与用户输入query相似的片段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768\n",
      "DeepLearning-Ng\n",
      "13-4-8 -Ufldl\n",
      "deeplearning.stanford.edu/wiki/index.php/ #.E4.B8.AD.E6.96.87.E8.AF.91.E8.80.85\n",
      "wbx5/6\n",
      "要求解这样的神经网络，需要样本集\n",
      " ，其中\n",
      " 。如果你想预测的输出是多\n",
      "个的，那这种神经网络很适用。（比如，在医疗诊断应用中，患者的体征指标就可以作为向量的输\n",
      "入值，而不同的输出值\n",
      "可以表示不同的疾病存在与否。）\n",
      "中英文对照\n",
      "neuralnetworks神经网络\n",
      "activationfunction激活函数\n",
      "hyperbolictangent双曲正切函数\n",
      "biasunits偏置项\n",
      "activation激活值\n",
      "forwardpropagation前向传播\n",
      "feedforwardneuralnetwork前馈神经网络(参照Mitchell的《机器学习》的翻译)\n",
      "中文译者\n",
      "孙逊（sunpaofu@foxmail.com），林锋（xlfg@yeah.net），刘鸿鹏飞\n",
      "（just.dark@foxmail.com）,许利杰（csxulijie@gmail.com）\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"什么是神经网络？\"\n",
    "embedding_vector = core_embeddings_model.embed_query(query)\n",
    "print(len(embedding_vector))\n",
    "#\n",
    "docs_resp = vectorstore.similarity_search_by_vector(embedding_vector, k=1)\n",
    "#\n",
    "for page in docs_resp:\n",
    "  print(page.page_content)\n",
    "  print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用集成检索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "ensemble_retriever = EnsembleRetriever(retrievers=[bm25_retriever, faiss_retriever],\n",
    "                                       weights=[0.5, 0.5])  # 每个检索器的权重"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 加载大模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is automatically converting to bf16 for faster inference. If you want to disable the automatic precision, please manually add bf16/fp16/fp32=True to \"AutoModelForCausalLM.from_pretrained\".\n",
      "Try importing flash-attention for faster inference...\n",
      "Warning: import flash_attn rotary fail, please install FlashAttention rotary to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/rotary\n",
      "Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm\n",
      "Warning: import flash_attn fail, please install FlashAttention to get higher efficiency https://github.com/Dao-AILab/flash-attention\n",
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:29<00:00,  3.74s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# device = torch.device(\"cuda:2\")\n",
    "# 提前下载到本地的大模型\n",
    "model_name_or_path = \"/home/work/var/data/ssr-share-data/Qwen-7B-Chat\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path, device_map=\"auto\", trust_remote_code=True).eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True, trust_remote_code=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 创建pipeline并初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'QWenLMHeadModel' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    do_sample=True,\n",
    "    temperature=0.1,\n",
    "    top_p=0.95,\n",
    "    top_k=40,\n",
    "    repetition_penalty=1.1\n",
    ")\n",
    "\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 设置缓存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "langchain.llm_cache = InMemoryCache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 设定输入提示模板"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = '''\n",
    "你是我的学术导师。你对深度学习领域知识非常擅长，能够给我提供关于深度学习、大模型相关的知识和建议。\n",
    "根据所提供的信息，尝试回答我提出的问题。\n",
    "如果你无法根据信息回答问题，请说“没有答案”。\n",
    "因此，在尝试深入理解上下文并进根据所提供的信息进行回答，不要生成不相关的答案。\n",
    "\n",
    "上下文: {context}\n",
    "问题: {question}\n",
    "\n",
    "答案:\n",
    "'''\n",
    "#\n",
    "input_variables = ['context', 'question']\n",
    "#\n",
    "custom_prompt = PromptTemplate(template=PROMPT_TEMPLATE,\n",
    "                            input_variables=input_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 检索chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "handler = StdOutCallbackHandler()\n",
    "\n",
    "qa_with_sources_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5}),\n",
    "    verbose=True,\n",
    "    callbacks=[handler],\n",
    "    chain_type_kwargs={\"prompt\": custom_prompt},\n",
    "    return_source_documents=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Response generated : \n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Source Documents : \n",
      " [Document(page_content='DeepLearning-Ng\\n13-4-8 -Ufldl\\ndeeplearning.stanford.edu/wiki/index.php/ #.E4.B8.AD.E6.96.87.E8.AF.91.E8.80.85\\nwbx5/6\\n要求解这样的神经网络，需要样本集\\n ，其中\\n 。如果你想预测的输出是多\\n个的，那这种神经网络很适用。（比如，在医疗诊断应用中，患者的体征指标就可以作为向量的输\\n入值，而不同的输出值\\n可以表示不同的疾病存在与否。）\\n中英文对照\\nneuralnetworks神经网络\\nactivationfunction激活函数\\nhyperbolictangent双曲正切函数\\nbiasunits偏置项\\nactivation激活值\\nforwardpropagation前向传播\\nfeedforwardneuralnetwork前馈神经网络(参照Mitchell的《机器学习》的翻译)\\n中文译者\\n孙逊（sunpaofu@foxmail.com），林锋（xlfg@yeah.net），刘鸿鹏飞\\n（just.dark@foxmail.com）,许利杰（csxulijie@gmail.com）', metadata={'source': '/home/work/project/MockingBird-main/pdf_data/deep-learning.pdf', 'page': 8}), Document(page_content='DeepLearning-Ng\\n13-4-8 -Ufldl\\ndeeplearning.stanford.edu/wiki/index.php/ #.E4.B8.AD.E6.96.87.E8.AF.91.E8.80.85\\nwbx3/6\\n，我们将第层记为\\n神经网络模型\\n所谓神经网络就是将许多个单一“神经元”联结在一起，这样，一个“神经元”的输出就可以是另\\n一个“神经元”的输入。例如，下图就是一个简单的神经网络：\\n我们使用圆圈来表示神经网络的输入，标上\\n”的圆圈被称为偏置节点，也就是截距项。神经\\n网络最左边的一层叫做输入层，最右的一层叫做输出层（本例中，输出层只有一个节点）。中间\\n所有节点组成的一层叫做隐藏层，因为我们不能在训练样本集中观测到它们的值。同时可以看\\n到，以上神经网络的例子中有3个输入单元（偏置单元不计在内），3个隐藏单元及一个输出单\\n元。\\n我们用来表示网络的层数，本例中 ，于是是输入层，\\n输出层是\\n。本例神经网络有参数 ，其中 （下面\\n的式子中用到）是第\\n层第单元与第\\n 层第\\n单元之间的联接参数（其实就是连接线上\\n的权重，注意标号顺序），是第\\n 层第', metadata={'source': '/home/work/project/MockingBird-main/pdf_data/deep-learning.pdf', 'page': 6}), Document(page_content='所有节点组成的一层叫做隐藏层，因为我们不能在训练样本集中观测到它们的值。同时可以看\\n到，以上神经网络的例子中有3个输入单元（偏置单元不计在内），3个隐藏单元及一个输出单\\n元。\\n我们用来表示网络的层数，本例中 ，于是是输入层，\\n输出层是\\n。本例神经网络有参数 ，其中 （下面\\n的式子中用到）是第\\n层第单元与第\\n 层第\\n单元之间的联接参数（其实就是连接线上\\n的权重，注意标号顺序），是第\\n 层第\\n单元的偏置项。因此在本例中，\\n， 。注意，没有其他单元连向偏置单元(即偏置单元没有输入)，\\n因为它们总是输出。同时，我们用\\n表示第\\n层的节点数（偏置单元不计在内）。\\n我们用\\n表示第层第单元的激活值（输出值）。当 时，\\n ，也就是第\\n个输入值（输入值的第\\n个特征）。对于给定参数集合\\n ，我们的神经网络就可以按照函\\n数来计算输出结果。本例神经网络的计算步骤如下：', metadata={'source': '/home/work/project/MockingBird-main/pdf_data/deep-learning.pdf', 'page': 6}), Document(page_content='后，第\\n 层的激活值 就可以按照下面步骤计算得到：\\n将参数矩阵化，使用矩阵－向量运算方式，我们就可以利用线性代数的优势对神经网络进行快速求\\n解。\\n目前为止，我们讨论了一种神经网络，我们也可以构建另一种结构的神经网络（这里结构指的是\\n神经元之间的联接模式），也就是包含多个隐藏层的神经网络。最常见的一个例子是\\n层的神经\\n网络，第\\n层是输入层，第层是输出层，中间的每个层\\n与层\\n 紧密相联。这种模式\\n下，要计算神经网络的输出结果，我们可以按照之前描述的等式，按部就班，进行前向传播，逐一\\n计算第\\n层的所有激活值，然后是第\\n层的激活值，以此类推，直到第\\n层。这是一个前\\n馈神经网络的例子，因为这种联接图没有闭环或回路。\\n神经网络也可以有多个输出单元。比如，下面的神经网络有两层隐藏层：\\n及\\n，输出层\\n有两个输出单元。', metadata={'source': '/home/work/project/MockingBird-main/pdf_data/deep-learning.pdf', 'page': 7}), Document(page_content='activation激活值\\nforwardpropagation前向传播\\nfeedforwardneuralnetwork前馈神经网络(参照Mitchell的《机器学习》的翻译)\\n中文译者\\n孙逊（sunpaofu@foxmail.com），林锋（xlfg@yeah.net），刘鸿鹏飞\\n（just.dark@foxmail.com）,许利杰（csxulijie@gmail.com）\\n神经网络|反向传导算法|梯度检验与高级优化|自编码算法与稀疏性|可视化自编码器训练结果|稀疏自编码\\n器符号一览表|Exercise:Sparse_Autoencoder', metadata={'source': '/home/work/project/MockingBird-main/pdf_data/deep-learning.pdf', 'page': 8})]\n"
     ]
    }
   ],
   "source": [
    "query = \"什么是神经网络？\"\n",
    "response = qa_with_sources_chain({\"query\": query})\n",
    "print(f\"Response generated : \\n {response['result']}\")\n",
    "print(f\"Source Documents : \\n {response['source_documents']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用集成检索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "handler = StdOutCallbackHandler()\n",
    "\n",
    "qa_with_sources_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever = ensemble_retriever,\n",
    "    callbacks=[handler],\n",
    "    chain_type_kwargs={\"prompt\": custom_prompt},\n",
    "    return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Response generated : \n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Source Documents : \n",
      " [Document(page_content='DeepLearning-Ng\\n13-4-8 -Ufldl\\ndeeplearning.stanford.edu/wiki/index.php/ #.E4.B8.AD.E6.96.87.E8.AF.91.E8.80.85\\nwbx5/6\\n要求解这样的神经网络，需要样本集\\n ，其中\\n 。如果你想预测的输出是多\\n个的，那这种神经网络很适用。（比如，在医疗诊断应用中，患者的体征指标就可以作为向量的输\\n入值，而不同的输出值\\n可以表示不同的疾病存在与否。）\\n中英文对照\\nneuralnetworks神经网络\\nactivationfunction激活函数\\nhyperbolictangent双曲正切函数\\nbiasunits偏置项\\nactivation激活值\\nforwardpropagation前向传播\\nfeedforwardneuralnetwork前馈神经网络(参照Mitchell的《机器学习》的翻译)\\n中文译者\\n孙逊（sunpaofu@foxmail.com），林锋（xlfg@yeah.net），刘鸿鹏飞\\n（just.dark@foxmail.com）,许利杰（csxulijie@gmail.com）', metadata={'source': '/home/work/project/MockingBird-main/pdf_data/deep-learning.pdf', 'page': 8}), Document(page_content='老铁没毛病，快上车！', metadata={'source': '/home/work/project/MockingBird-main/pdf_data/deep-learning.pdf', 'page': 0}), Document(page_content='Step5:Testclassifier\\nNowthatyouhaveatrainedsoftmaxclassifier,youcanseehowwellitperformson\\nthetestset.Thesepooledfeaturesforthetestsetwillberunthroughthesoftmax\\nclassifier,andtheaccuracyofthepredictionswillbecomputed.Youshouldexpect\\ntogetanaccuracyofaround80%.\\nRetrievedfrom\\n\"http://deeplearning.stanford.edu/wiki/index.php/Exercise:Convolution_and_Pooling\"\\nThispagewaslastmodifiedon3June2011,at19:16.', metadata={'source': '/home/work/project/MockingBird-main/pdf_data/deep-learning.pdf', 'page': 116}), Document(page_content='DeepLearning-Ng\\n13-4-8 -Ufldl\\ndeeplearning.stanford.edu/wiki/index.php/ #.E4.B8.AD.E6.96.87.E8.AF.91.E8.80.85\\nwbx3/6\\n，我们将第层记为\\n神经网络模型\\n所谓神经网络就是将许多个单一“神经元”联结在一起，这样，一个“神经元”的输出就可以是另\\n一个“神经元”的输入。例如，下图就是一个简单的神经网络：\\n我们使用圆圈来表示神经网络的输入，标上\\n”的圆圈被称为偏置节点，也就是截距项。神经\\n网络最左边的一层叫做输入层，最右的一层叫做输出层（本例中，输出层只有一个节点）。中间\\n所有节点组成的一层叫做隐藏层，因为我们不能在训练样本集中观测到它们的值。同时可以看\\n到，以上神经网络的例子中有3个输入单元（偏置单元不计在内），3个隐藏单元及一个输出单\\n元。\\n我们用来表示网络的层数，本例中 ，于是是输入层，\\n输出层是\\n。本例神经网络有参数 ，其中 （下面\\n的式子中用到）是第\\n层第单元与第\\n 层第\\n单元之间的联接参数（其实就是连接线上\\n的权重，注意标号顺序），是第\\n 层第', metadata={'source': '/home/work/project/MockingBird-main/pdf_data/deep-learning.pdf', 'page': 6}), Document(page_content='所有节点组成的一层叫做隐藏层，因为我们不能在训练样本集中观测到它们的值。同时可以看\\n到，以上神经网络的例子中有3个输入单元（偏置单元不计在内），3个隐藏单元及一个输出单\\n元。\\n我们用来表示网络的层数，本例中 ，于是是输入层，\\n输出层是\\n。本例神经网络有参数 ，其中 （下面\\n的式子中用到）是第\\n层第单元与第\\n 层第\\n单元之间的联接参数（其实就是连接线上\\n的权重，注意标号顺序），是第\\n 层第\\n单元的偏置项。因此在本例中，\\n， 。注意，没有其他单元连向偏置单元(即偏置单元没有输入)，\\n因为它们总是输出。同时，我们用\\n表示第\\n层的节点数（偏置单元不计在内）。\\n我们用\\n表示第层第单元的激活值（输出值）。当 时，\\n ，也就是第\\n个输入值（输入值的第\\n个特征）。对于给定参数集合\\n ，我们的神经网络就可以按照函\\n数来计算输出结果。本例神经网络的计算步骤如下：', metadata={'source': '/home/work/project/MockingBird-main/pdf_data/deep-learning.pdf', 'page': 6}), Document(page_content='Step4:Usepooledfeaturesforclassification\\nInthisstep,youwillusethepooledfeaturestotrainasoftmaxclassifiertomap\\nthepooledfeaturestotheclasslabels.ThecodeinthissectionusessoftmaxTrain\\nfromthesoftmaxexercisetotrainasoftmaxclassifieronthepooledfeaturesfor\\n500iterations,whichshouldtakearoundafewminutes.\\nStep5:Testclassifier\\nNowthatyouhaveatrainedsoftmaxclassifier,youcanseehowwellitperformson\\nthetestset.Thesepooledfeaturesforthetestsetwillberunthroughthesoftmax', metadata={'source': '/home/work/project/MockingBird-main/pdf_data/deep-learning.pdf', 'page': 116}), Document(page_content='后，第\\n 层的激活值 就可以按照下面步骤计算得到：\\n将参数矩阵化，使用矩阵－向量运算方式，我们就可以利用线性代数的优势对神经网络进行快速求\\n解。\\n目前为止，我们讨论了一种神经网络，我们也可以构建另一种结构的神经网络（这里结构指的是\\n神经元之间的联接模式），也就是包含多个隐藏层的神经网络。最常见的一个例子是\\n层的神经\\n网络，第\\n层是输入层，第层是输出层，中间的每个层\\n与层\\n 紧密相联。这种模式\\n下，要计算神经网络的输出结果，我们可以按照之前描述的等式，按部就班，进行前向传播，逐一\\n计算第\\n层的所有激活值，然后是第\\n层的激活值，以此类推，直到第\\n层。这是一个前\\n馈神经网络的例子，因为这种联接图没有闭环或回路。\\n神经网络也可以有多个输出单元。比如，下面的神经网络有两层隐藏层：\\n及\\n，输出层\\n有两个输出单元。', metadata={'source': '/home/work/project/MockingBird-main/pdf_data/deep-learning.pdf', 'page': 7}), Document(page_content='pooledfeaturesforbothtrainingandtestsets.Thepooledfeaturesforthe\\ntrainingsetwillbeusedtotrainyourclassifier,whichyoucanthentestonthe\\ntestset.\\nBecausetheconvolvedfeaturesmatrixisverylarge,thecodeprovideddoesthe\\nconvolutionandpooling50featuresatatimetoavoidrunningoutofmemory.\\nStep4:Usepooledfeaturesforclassification\\nInthisstep,youwillusethepooledfeaturestotrainasoftmaxclassifiertomap\\nthepooledfeaturestotheclasslabels.ThecodeinthissectionusessoftmaxTrain', metadata={'source': '/home/work/project/MockingBird-main/pdf_data/deep-learning.pdf', 'page': 116}), Document(page_content='activation激活值\\nforwardpropagation前向传播\\nfeedforwardneuralnetwork前馈神经网络(参照Mitchell的《机器学习》的翻译)\\n中文译者\\n孙逊（sunpaofu@foxmail.com），林锋（xlfg@yeah.net），刘鸿鹏飞\\n（just.dark@foxmail.com）,许利杰（csxulijie@gmail.com）\\n神经网络|反向传导算法|梯度检验与高级优化|自编码算法与稀疏性|可视化自编码器训练结果|稀疏自编码\\n器符号一览表|Exercise:Sparse_Autoencoder', metadata={'source': '/home/work/project/MockingBird-main/pdf_data/deep-learning.pdf', 'page': 8}), Document(page_content='DeepLearning-Ng\\n13-4-8 Exercise:ConvolutionandPooling-Ufldl\\ndeeplearning.stanford.edu/wiki/index.php/Exercise:Convolution_and_Pooling 5/5\\nwbxInthisstep,youwillconvolveeachofthefeaturesyoulearnedwiththefull64x64\\nimagesfromtheSTL-10datasettoobtaintheconvolvedfeaturesforboththe\\ntrainingandtestsets.Youwillthenpooltheconvolvedfeaturestoobtainthe\\npooledfeaturesforbothtrainingandtestsets.Thepooledfeaturesforthe\\ntrainingsetwillbeusedtotrainyourclassifier,whichyoucanthentestonthe\\ntestset.', metadata={'source': '/home/work/project/MockingBird-main/pdf_data/deep-learning.pdf', 'page': 116})]\n"
     ]
    }
   ],
   "source": [
    "query = \"什么是神经网络？\"\n",
    "response = qa_with_sources_chain({\"query\":query})\n",
    "print(f\"Response generated : \\n {response['result']}\")\n",
    "print(f\"Source Documents : \\n {response['source_documents']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
